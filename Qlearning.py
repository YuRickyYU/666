# -*- coding: utf-8 -*-
"""Multi-agent reinforcement learning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MLqzMKJmBa74UOsTxhIgvHZhQodDbz9K
"""

import copy
import random
from abc import ABC, abstractmethod
from enum import Enum
from typing import List
from typing import Tuple
from matplotlib.widgets import Button
import matplotlib.cm as cm
import random as rd
import scipy as sp
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import numpy as np
import torch
import time, threading
matplotlib.use("Agg")

"""The Coordinate class is used to represent coordinate points and provides common operation methods."""

class Coordinate:
    def __init__(self, x: int, y: int):
        self.x = x
        self.y = y

    def __add__(self, other: 'Coordinate') -> 'Coordinate':
        if isinstance(other, Coordinate):
            return Coordinate(self.x + other.x, self.y + other.y)
        return NotImplemented

    def __sub__(self, other: 'Coordinate') -> 'Coordinate':
        if isinstance(other, Coordinate):
            return Coordinate(self.x - other.x, self.y - other.y)
        return NotImplemented

    def __repr__(self) -> str:
        return f"Coordinate(x={self.x}, y={self.y})"

    def __str__(self) -> str:
        return f"Coordinate({self.x}, {self.y})"

    def __eq__(self, other: 'Coordinate') -> bool:
        if isinstance(other, Coordinate):
            return self.x == other.x and self.y == other.y
        return False

    def __hash__(self):
        return hash((self.x, self.y))

    def to_tuple(self) -> Tuple[int, int]:
        return (self.x, self.y)

    @staticmethod
    def from_tuple(coord: Tuple[int, int]) -> 'Coordinate':
        return Coordinate(coord[0], coord[1])

"""Three Enum classes and some operations related to coordinates and actions, suitable for the agent's actions and reward mechanism."""

class Item(Enum):
    NOT_IN_POSSESSION = 0
    IN_POSSESSION = 1


class Reward(Enum):
    EXCHANGE_KEY = 50
    DROPPED_OFF_ITEM = 50
    WANDERED = -1
    BUMPED = 0


class Action(Enum):
    NORTH = 0
    EAST = 1
    SOUTH = 2
    WEST = 3

    def to_coordinate(self):
        if self == Action.NORTH:
            return Coordinate(0, -1)
        elif self == Action.EAST:
            return Coordinate(1, 0)
        elif self == Action.SOUTH:
            return Coordinate(0, 1)
        else:
            return Coordinate(-1, 0)

    @staticmethod
    def generate_random_action():
        return random.choice(list(Action))

"""
An agent that knows its location and the world it's in."""

class Agent(ABC):
    def __init__(self, starting_coord: Coordinate, agent_id, agent_type):
        self.location = starting_coord
        self.agent_id = agent_id
        self.agent_type = agent_type
        self.has_item = True
        self.has_half_key = True
        self.has_full_key = False
        self.objective_completed = False
        self.world = None
        self.bumped_wall = False
        self.exchange_key = False

    def move(self, action: Action):
        new_location = self.location + action.to_coordinate()
        if self.within_world_boundary(new_location):
            self.location = new_location
        else:
            self.bumped_wall = True

    def spawn(self, location: Coordinate):
        self.location = location

    def within_world_boundary(self, location: Coordinate) -> bool:
        return self.world.valid_location(location)

    def has_completed_objective(self) -> bool:
        return self.objective_completed

    def observe_world(self, world: 'MultiAgentGridWorld' = None) -> None:
        if world:
            self.world = world

    @abstractmethod
    def reset(self) -> None:
        pass

    @abstractmethod
    def get_action(self) -> Action:
        pass

    @abstractmethod
    def execute_action(self, action: Action) -> None:
        pass

    @abstractmethod
    def activate_testing_mode(self):
        pass

    @abstractmethod
    def activate_training_mode(self):
        pass

"""Grid world with multiple agent in it."""

class MultiAgentGridWorld(ABC):
    def __init__(self, agent_list: list[Agent], n: int):
        self.n = n
        self.agent_list = agent_list
        self.blocked_locations: List[Coordinate] = []

    def valid_location(self, coord: Coordinate) -> bool:
        x, y = coord.to_tuple()
        not_blocked_cell = coord not in self.blocked_locations
        in_boundary = (0 <= x < self.n) and (0 <= y < self.n)  # within grid world dimensions
        return not_blocked_cell and in_boundary

    @staticmethod
    def generate_random_coord(n: int) -> Coordinate:
        rows = n
        cols = n
        x = np.random.randint(0, cols)
        y = np.random.randint(0, rows)
        return Coordinate(x, y)

    @abstractmethod
    def has_agents_completed_objective(self) -> bool:
        pass

    @abstractmethod
    def reset(self) -> None:
        pass

    @abstractmethod
    def activate_testing_mode(self):
        pass

    @abstractmethod
    def activate_training_mode(self):
        pass

"""Deep Q Learning Algorithm

The function "prepare_torch" needs to be called once and only once at the start of your program to initialise PyTorch and generate the two Q-networks. It returns the target model (for testing).

The function "update_target" copies the state of the prediction network to the target network. You need to use this in regular intervals.

The function "get_qvals" returns a numpy list of qvals for the state given by the argument _based on the prediction network_.

The function "get_maxQ" returns the maximum q-value for the state given by the argument _based on the target network_.

The function "train_one_step_new" performs a single training step. It returns the current loss (only needed for debugging purposes). Its parameters are three parallel lists: a minibatch of states, a minibatch of actions, a minibatc
"""

from collections import deque


class DeepQLearning:
    def __init__(self,
                 statespace_size: int,
                 learning_rate: float = 0.997,
                 starting_epsilon: float = 1.0,
                 epsilon_decay_factor: float = 0.997,
                 min_epsilon: float = 0.1,
                 replay_buffer_size: int = 1000,
                 batch_size: int = 200,
                 network_copy_freq: int = 500):
        self.statespace_size = statespace_size
        self.learning_rate = learning_rate
        self.epsilon = starting_epsilon
        self.epsilon_decay_factor = epsilon_decay_factor
        self.min_epsilon = min_epsilon
        self.replay_buffer_size = replay_buffer_size
        self.batch_size = batch_size
        self.network_copy_freq = network_copy_freq
        self.model = None
        self.model2 = None
        self.optimizer = None
        self.loss_fn = None
        self.replay_buffer = deque(maxlen=self.replay_buffer_size)
        self.use_epsilon = True
        self.loss_history = []

    def get_action(self, state) -> Action:
        if self.use_epsilon and random.random() < self.epsilon:
            return Action.generate_random_action()
        else:
            return Action(self.get_maxQIndex(state).item())

    def ignore_epsilon(self) -> None:
        self.use_epsilon = False

    def acknowledge_epsilon(self) -> None:
        self.use_epsilon = True

    def addToReplay(self, current_state, action, reward, next_state, done) -> None:
        self.replay_buffer.append((current_state, action, reward, next_state, done))

    def decay_epsilon(self) -> None:
        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay_factor)

    def enough_experiences_in_buffer(self):
        return len(self.replay_buffer) >= self.batch_size

    def prepare_torch(self):
        l1 = self.statespace_size
        l2 = 150
        l3 = 100
        l4 = 4
        self.model = torch.nn.Sequential(
            torch.nn.Linear(l1, l2),
            torch.nn.ReLU(),
            torch.nn.Linear(l2, l3),
            torch.nn.ReLU(),
            torch.nn.Linear(l3, l4))
        self.model2 = copy.deepcopy(self.model)  # target model
        self.model2.load_state_dict(self.model.state_dict())
        self.loss_fn = torch.nn.MSELoss()
        learning_rate = 1e-3
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=learning_rate)
        return self.model2

    def update_target(self):
        # update parameter of target model
        self.model2.load_state_dict(self.model.state_dict())

    def get_qvals(self, state):
        # return Qval from prediction model
        state1 = torch.from_numpy(state).float()
        qvals_torch = self.model(state1)
        qvals = qvals_torch.data.numpy()
        return qvals

    def get_qvals2(self, state):
        # return Qval from target model
        state1 = torch.from_numpy(state).float()
        qvals_torch = self.model2(state1)
        qvals = qvals_torch.data.numpy()
        return qvals

    def get_maxQ(self, s):
        # return the maximum Q value from target model
        return torch.max(self.model2(torch.from_numpy(s).float())).float()

    def get_maxQIndex(self, s):
        # return the index of the maximum Qvalue from target model
        return torch.argmax(self.model2(torch.from_numpy(s).float()))

    def train_one_step(self, states, actions, targets, gamma=0.95):
        # pass to this function: state1_batch, action_batch, TD_batch
        global model, model2
        state1_batch = torch.cat([torch.from_numpy(s).float() for s in states])
        action_batch = torch.Tensor(actions)
        Q1 = self.model(state1_batch)
        X = Q1.gather(dim=1, index=action_batch.long().unsqueeze(dim=1)).squeeze()
        Y = torch.tensor(targets)
        loss = self.loss_fn(X, Y)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        return loss.item()

    def learn(self, steps: int, gamma=0.95):
        # perform forward propagation for a random minibatch sampled from replay_buffer
        mini_batch = random.sample(self.replay_buffer, self.batch_size)
        loss = 0
        targets = []
        states = []
        actions = []
        for cur_state, action, reward, next_state, done in mini_batch:
            if done:
                y = reward.value
            else:
                y = reward.value + gamma * self.get_maxQ(next_state).item()
            targets.append(y)
            states.append(np.array(cur_state).reshape(1, 4))
            actions.append(action.value)

        # calculate the loss of one step of training
        loss = self.train_one_step(states, actions, targets)
        self.loss_history.append(loss / self.batch_size)

        # copy weights from model to model2 (target) every self.network_copy_freq steps
        if steps % self.network_copy_freq == 0:
            # print("Networks copied")
            self.update_target()

    def get_loss_history(self):
        return self.loss_history

"""DeepQLearningAgent
Agents learning through q learning exchange keys then drop it off at location B.
"""

class DeepQLearningAgent(Agent):
    def __init__(self, starting_coord: Coordinate, agent_id, agent_type, q_learning: DeepQLearning):
        super().__init__(starting_coord, agent_id, agent_type)
        self.location_target = None
        self.q_learning = q_learning
        self.acc_reward = 0
        self.training_mode = True

    def get_state(self) -> np.array:
        return np.array([self.location.x, self.location.y,
                         self.location_target.x, self.location_target.y], dtype=int)

    def get_reward(self) -> Reward:
        if self.location == self.location_target and self.has_full_key:
            self.has_full_key = False
            self.exchange_key = False
            self.objective_completed = True
            return Reward.DROPPED_OFF_ITEM
        elif self.exchange_key:
            self.exchange_key = False
            return Reward.EXCHANGE_KEY
        elif self.bumped_wall:
            self.bumped_wall = False
            return Reward.BUMPED
        else:
            self.objective_completed = False
            return Reward.WANDERED

    def add_reward(self, type: Reward) -> None:
        self.acc_reward += type.value

    def activate_training_mode(self) -> None:
        self.training_mode = True
        self.q_learning.acknowledge_epsilon()

    def activate_testing_mode(self) -> None:
        self.training_mode = False
        self.q_learning.ignore_epsilon()

    def observe_world(self, world: MultiAgentGridWorld = None) -> None:
        super().observe_world(world)
        self.location_target = self.world.location_target

    def get_action(self) -> Action:
        state = self.get_state()
        return self.q_learning.get_action(state)

    @staticmethod
    def is_meet(agent_list: List[Agent]):
        position_list = [agent.location for agent in agent_list]
        for i in range(len(position_list)):
            for j in range(i + 1, len(position_list)):
                if position_list[i] == position_list[j]:
                    return True
        return False

    @staticmethod
    def exchange_key(agent_list: List[Agent]):
        position_list = [agent.location for agent in agent_list]
        index_map = {}
        for index, element in enumerate(position_list):
            if element not in index_map:
                index_map[element] = []
            index_map[element].append(index)
        meet = {key: value for key, value in index_map.items() if len(value) > 1}
        if len(meet.keys()) == 2:
            for group in meet.values():
                if agent_list[group[0]].agent_type != agent_list[group[1]].agent_type:
                    agent_list[group[0]].exchange_key = True
                    agent_list[group[1]].exchange_key = True
                    agent_list[group[0]].has_full_key = True
                    agent_list[group[1]].has_full_key = True
            return
        if len(meet.keys()) == 1:
            if len(meet.values()) == 3:
                for index in list(meet.values())[0]:
                    agent_list[index].exchange_key = True
                    agent_list[index].has_full_key = True
                return
            if len(meet.values()) == 2:
                index1, index2 = list(meet.values())[0]
                if agent_list[index1].agent_type != agent_list[index2].agent_type:
                    agent_list[index1].exchange_key = True
                    agent_list[index2].exchange_key = True
                    agent_list[index1].has_full_key = True
                    agent_list[index2].has_full_key = True
                return
            if len(meet.values()) == 4:
                for index in list(meet.values())[0]:
                    agent_list[index].exchange_key = True
                    agent_list[index].has_full_key = True

    def execute_action(self, action: Action) -> None:
        current_state = self.get_state()  # S # R
        self.move(action)  # take action A
        if self.is_meet(self.world.agent_list):
            DeepQLearningAgent.exchange_key(self.world.agent_list)
        next_state = self.get_state()  # S'
        reward = self.get_reward()  # R
        self.add_reward(reward)

        if self.training_mode:
            self.q_learning.addToReplay(current_state, action, reward, next_state, self.objective_completed)

    def reset(self) -> None:
        self.objective_completed = False
        self.acc_reward = 0
        self.observe_world()

    def set_location(self, location: Coordinate) -> None:
        self.location = location

    def sample(self) -> None:
        self.q_learning.decay_epsilon()
        action = self.get_action()
        self.execute_action(action)

    def learn(self, steps: int) -> None:
        self.q_learning.learn(steps)

"""A `MultiAgentGridWorld` with four agents and location B"""

class ABMultiAgentGridWorld(MultiAgentGridWorld):
    def __init__(self, agent_list: list[DeepQLearningAgent], n: int) -> None:
        super().__init__(agent_list, n)
        self.location_target = self._generate_random_valid_coord()
        self.agent_list = agent_list
        for agent in agent_list:
            agent.observe_world(self)
        self.steps = 0

    def _generate_random_valid_coord(self, invalid_coords: List[Coordinate] = []) -> Coordinate:
        while True:
            coord = self.generate_random_coord(self.n)
            if coord not in invalid_coords:
                return coord

    def spawn_agent(self, location: Coordinate = None):
        if location:
            for agent in self.agent_list:
                agent.set_location(location)
        else:
            for agent in self.agent_list:
                agent.set_location(self._generate_random_valid_coord())
    def step(self):
        for agent in self.agent_list:
           agent.sample()  # decay epsilon, get current state, execute action, and add agent reward

        for agent in self.agent_list:
          if agent.q_learning.enough_experiences_in_buffer():
             agent.learn(self.steps)  # 只有当经验池中有足够的经验时，才进行学习
        self.steps += 1

        return self.has_agents_completed_objective()

    def has_agents_completed_objective(self):
        position_list = [agent.location for agent in self.agent_list]
        in_target_location = []
        for index, position in enumerate(position_list):
            if position == self.location_target:
                in_target_location.append(index)
        for index in in_target_location:
            if self.agent_list[index].has_full_key:
                return True
        return False

    def reset(self, agent_location_list: List[Coordinate] = None, location_target: Coordinate = None):
        # resets location A and location B and sets agent
        self.location_target = location_target if location_target else self._generate_random_valid_coord()
        for agent_location in agent_location_list:
            self.spawn_agent(agent_location)
        for agent in self.agent_list:
            agent.reset()

    def set_agent_location(self, location: Coordinate):
        for agent in self.agent_list:
            agent.location = location

    def activate_testing_mode(self):
        for agent in self.agent_list:
            agent.activate_testing_mode()

    def activate_training_mode(self):
        for agent in self.agent_list:
            agent.activate_training_mode()

"""Training Model
The agent will be trained for a given number of episodes.
"""

def train_agent(grid_world: ABMultiAgentGridWorld, num_episodes: int) -> None:
    # set training mode to true for greedy epsilon policy improvement
    grid_world.activate_training_mode()

    for ep in range(num_episodes):
        position_list = [0, 1, 2, 3]
        while True:
            pos = []
            for i in range(10):
                p = random.choice(position_list)
                pos.append(p)
            x1, x2, x3, x4, x5, y1, y2, y3, y4, y5 = pos
            if (x1, y1) != (x2, y2) != (x3, y3) != (x4, y4):
                print('init position OK')
                break
        grid_world.reset(agent_location_list=
                         [Coordinate(x1, y1), Coordinate(x2, y2), Coordinate(x3, y3),
                          Coordinate(x4, y4)],
                         location_target=Coordinate(x5, y5))
        objective_completed = False
        while not objective_completed:
            objective_completed = grid_world.step()
        print(f"Episode {ep + 1} completed. ")
    agent_list = grid_world.agent_list
    all_agents_loss_history = []
    for agent in agent_list:
        loss_history = agent.q_learning.get_loss_history()
        all_agents_loss_history.append(loss_history)  # [[], [], [], []]
    average_loss_per_step = [sum(losses) / 4 for losses in zip(*all_agents_loss_history)]  # [int, int, int, int]

    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(range(len(average_loss_per_step)), average_loss_per_step, label='Total Loss')
    plt.xlabel('Steps')
    plt.ylabel('Loss')
    plt.title('Loss Progress During Training')
    plt.legend()
    plt.show()

def test_agent(grid_world: ABMultiAgentGridWorld) -> None:
    steps_per_test = []  # Store the number of steps for each test
    failed = 0

    grid_world.activate_testing_mode()

    position_list = [0, 1, 2, 3]
    while True:
        pos = []
        for _ in range(10):
            p = random.choice(position_list)
            pos.append(p)
        x1, x2, x3, x4, x5, y1, y2, y3, y4, y5 = pos
        if (x1, y1) != (x2, y2) != (x3, y3) != (x4, y4):
            print('init position OK')
            break
    grid_world.reset(agent_location_list=
                     [Coordinate(x1, y1), Coordinate(x2, y2), Coordinate(x3, y3),
                      Coordinate(x4, y4)],
                     location_target=Coordinate(x5, y5))
    step_count = 0

    for agent in grid_world.agent_list:
        agent.q_learning.epsilon = 0
    while not grid_world.has_agents_completed_objective() and step_count < 10000:
        for agent in grid_world.agent_list:
            action = agent.get_action()
            agent.execute_action(action)
            step_count += 1
    if step_count >= 10000:
        failed += 1
        print('Failed')
    steps_per_test.append(step_count)  # Record the number of steps
    print(steps_per_test)

def test_agent_with_animation(grid_world: ABMultiAgentGridWorld, num_episodes: int) -> None:
    fig, ax = plt.subplots()
    config = np.zeros([grid_world.n, grid_world.n])
    mat = ax.matshow(config, cmap=cm.seismic)
    ax.axis('on')
    plt.title("Q-learning test Visualization")

    agent_rep1, agent_rep2, agent_rep3, agent_rep4 = None, None, None, None
    destination_rep = None
    is_training = False
    stop = False

    def initialize_destination():
        nonlocal destination_rep
        dest_x = grid_world.location_target.x
        dest_y = grid_world.location_target.y
        destination_rep = plt.Rectangle((dest_x - 0.3, dest_y - 0.3), 0.6, 0.6, color='purple', fill=True, linewidth=2)
        ax.add_patch(destination_rep)

    def update_visualization():
        nonlocal agent_rep1, agent_rep2, agent_rep3, agent_rep4
        if agent_rep1:
            agent_rep1.remove()
        if agent_rep2:
            agent_rep2.remove()
        if agent_rep3:
            agent_rep3.remove()
        if agent_rep4:
            agent_rep4.remove()

        agent_rep1 = plt.Circle((grid_world.agent_list[0].location.x, grid_world.agent_list[0].location.y), 0.2,
                                color='yellow', fill=True, linewidth=2)
        agent_rep2 = plt.Circle((grid_world.agent_list[1].location.x, grid_world.agent_list[1].location.y), 0.2,
                                color='red', fill=True, linewidth=2)
        agent_rep3 = plt.Circle((grid_world.agent_list[2].location.x, grid_world.agent_list[2].location.y), 0.2,
                                color='green', fill=True, linewidth=2)
        agent_rep4 = plt.Circle((grid_world.agent_list[3].location.x, grid_world.agent_list[3].location.y), 0.2,
                                color='blue', fill=True, linewidth=2)
        ax.add_patch(agent_rep1)
        ax.add_patch(agent_rep2)
        ax.add_patch(agent_rep3)
        ax.add_patch(agent_rep4)

    def advance_training():
        nonlocal stop
        if stop:
            return
        objective_completed = False
        step_count = 0
        while not objective_completed and not stop and step_count < 10000:
            objective_completed = grid_world.step()
            update_visualization()
            step_count += 4
            plt.draw()
            plt.pause(0.1)
        failed = 0
        steps_per_test = []
        if step_count >= 10000:
            failed += 1
            print('Failed')
        steps_per_test.append(step_count)  # Record the number of steps
        print(steps_per_test)

    def start_training(i):
        if is_training:
            advance_training()

    def on_start(event):
        nonlocal is_training, stop
        is_training = True
        stop = False

    def on_stop(event):
        nonlocal stop
        stop = True

    def on_init(event):
        position_list = [0, 1, 2, 3]
        while True:
            pos = []
            for _ in range(10):
                p = random.choice(position_list)
                pos.append(p)
            x1, x2, x3, x4, x5, y1, y2, y3, y4, y5 = pos
            if (x1, y1) != (x2, y2) != (x3, y3) != (x4, y4):
                print('init position OK')
                break
        grid_world.reset(agent_location_list=
                         [Coordinate(x1, y1), Coordinate(x2, y2), Coordinate(x3, y3),
                          Coordinate(x4, y4)],
                         location_target=Coordinate(x5, y5))
        initialize_destination()
        update_visualization()
        plt.draw()

    def on_next(event):
        grid_world.step()
        update_visualization()
        plt.draw()

    axnext = plt.axes([0.85, 0.15, 0.1, 0.075])
    axstart = plt.axes([0.85, 0.25, 0.1, 0.075])
    axstop = plt.axes([0.85, 0.35, 0.1, 0.075])
    axinit = plt.axes([0.85, 0.45, 0.1, 0.075])

    bnext = Button(axnext, 'Next')
    bnext.on_clicked(on_next)

    bstart = Button(axstart, 'Start')
    bstart.on_clicked(on_start)

    bstop = Button(axstop, 'Stop')
    bstop.on_clicked(on_stop)

    binit = Button(axinit, 'Init')
    binit.on_clicked(on_init)


    ani = animation.FuncAnimation(fig, start_training, interval=100, blit=False)
    plt.show()


if __name__ == '__main__':
    """### Parameter setting and initialization for testing model"""

    statespace_size = 4
    learning_rate = 0.997
    starting_epsilon = 0.5
    epsilon_decay_factor = 0.995
    min_epsilon = 0.3
    replay_buffer_size = 1000
    batch_size = 100
    network_copy_freq = 500

    # Parameter Settings
    n = 5
    num_train_episodes = 1000  # Number of training episodes
    test_numbers = 100

    # Initialize the agent, Q-learning, and the environment
    q_learning_a = DeepQLearning(statespace_size,
                                 learning_rate,
                                 starting_epsilon,
                                 epsilon_decay_factor,
                                 min_epsilon,
                                 replay_buffer_size,
                                 batch_size,
                                 network_copy_freq)
    q_learning_b = DeepQLearning(statespace_size,
                                 learning_rate,
                                 starting_epsilon,
                                 epsilon_decay_factor,
                                 min_epsilon,
                                 replay_buffer_size,
                                 batch_size,
                                 network_copy_freq)
    q_learning_a.prepare_torch()
    # q_learning_b.prepare_torch()
    agent1 = DeepQLearningAgent(ABMultiAgentGridWorld.generate_random_coord(n), 1, 'A', q_learning_a)
    agent2 = DeepQLearningAgent(ABMultiAgentGridWorld.generate_random_coord(n), 2, 'A', q_learning_a)
    agent3 = DeepQLearningAgent(ABMultiAgentGridWorld.generate_random_coord(n), 3, 'B', q_learning_a)
    agent4 = DeepQLearningAgent(ABMultiAgentGridWorld.generate_random_coord(n), 4, 'B', q_learning_a)
    agent_list = [agent1, agent2, agent3, agent4]
    world = ABMultiAgentGridWorld(agent_list, n)
    train_agent(world, num_train_episodes)